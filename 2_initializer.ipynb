{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1= (10, 12288)\n",
      "b1= (10, 1)\n",
      "W2= (3, 10)\n",
      "b2= (3, 1)\n",
      "W3= (2, 3)\n",
      "b3= (2, 1)\n",
      "W4= (1, 2)\n",
      "b4= (1, 1)\n",
      "iters = 0  loss = 0.12190178942576657\n",
      "iters = 100  loss = 0.11308617542623937\n",
      "iters = 200  loss = 0.09732410387869979\n",
      "iters = 300  loss = 0.10704321766888293\n",
      "iters = 400  loss = 0.10057812412261836\n",
      "iters = 500  loss = 0.0696676590711824\n",
      "iters = 600  loss = 0.05719147003674176\n",
      "iters = 700  loss = 0.052768886766755334\n",
      "iters = 800  loss = 0.05294348441604328\n",
      "iters = 900  loss = 0.04687751787247037\n",
      "iters = 1000  loss = 0.01384060822452953\n",
      "iters = 1100  loss = 0.009789509301844452\n",
      "iters = 1200  loss = 0.008372867277582869\n",
      "iters = 1300  loss = 0.007546599206663303\n",
      "iters = 1400  loss = 0.007001308987785302\n",
      "iters = 1500  loss = 0.006618308224712877\n",
      "iters = 1600  loss = 0.006337023433389624\n",
      "iters = 1700  loss = 0.006122369052514992\n",
      "iters = 1800  loss = 0.005953589465174941\n",
      "iters = 1900  loss = 0.005817615661866748\n",
      "iters = 2000  loss = 0.0057057832017143135\n",
      "A1 mean: 0.26263064887108495 A1 std: 0.582785014306624\n",
      "dW 1 <e-8: 61525 total: 122880\n",
      "A2 mean: 0.1899274751648049 A2 std: 0.785233254629776\n",
      "dW 2 <e-8: 15 total: 30\n",
      "A3 mean: 0.9419797097155898 A3 std: 0.020338708742909273\n",
      "dW 3 <e-8: 4 total: 6\n",
      "A4 mean: 0.04171065926291567 A4 std: 0.0\n",
      "dW 4 <e-8: 0 total: 2\n",
      "iters = 2100  loss = 0.005612160605802399\n",
      "iters = 2200  loss = 0.005532573851570553\n",
      "iters = 2300  loss = 0.005464003963906017\n",
      "iters = 2400  loss = 0.00540420900263273\n",
      "iters = 2500  loss = 0.0053514817831487715\n",
      "iters = 2600  loss = 0.0053044897404747405\n",
      "iters = 2700  loss = 0.005262164932240191\n",
      "iters = 2800  loss = 0.00522362458178838\n",
      "iters = 2900  loss = 0.00518810922114221\n",
      "iters = 3000  loss = 0.005154928369217542\n",
      "iters = 3100  loss = 0.005123403169999459\n",
      "iters = 3200  loss = 0.005092789633198909\n",
      "iters = 3300  loss = 0.005062147033709046\n",
      "iters = 3400  loss = 0.00503005036461238\n",
      "iters = 3500  loss = 0.004993747453247075\n",
      "iters = 3600  loss = 0.004944793923500453\n",
      "iters = 3700  loss = 0.0031982241059360155\n",
      "iters = 3800  loss = 0.0029380770877550414\n",
      "iters = 3900  loss = 0.002881649518512513\n",
      "iters = 4000  loss = 0.0028436740592681273\n",
      "A1 mean: 0.25391085669236235 A1 std: 0.6587950994182523\n",
      "dW 1 <e-8: 61159 total: 122880\n",
      "A2 mean: 0.1786108038131732 A2 std: 0.8064483674781129\n",
      "dW 2 <e-8: 15 total: 30\n",
      "A3 mean: 0.9539202897572883 A3 std: 0.01701765938287869\n",
      "dW 3 <e-8: 4 total: 6\n",
      "A4 mean: 0.02748111635887514 A4 std: 0.0\n",
      "dW 4 <e-8: 0 total: 2\n",
      "iters = 4100  loss = 0.0028115735783689364\n",
      "iters = 4200  loss = 0.002781660669007372\n",
      "iters = 4300  loss = 0.0027505124619036665\n",
      "iters = 4400  loss = 0.0027068147672077007\n",
      "iters = 4500  loss = 0.0024857639813365934\n",
      "iters = 4600  loss = 0.0007248420491821607\n",
      "iters = 4700  loss = 0.0006008410074135363\n",
      "iters = 4800  loss = 0.0005437833300734086\n",
      "iters = 4900  loss = 0.0005046812319871057\n",
      "iters = 5000  loss = 0.0004742203945385653\n",
      "iters = 5100  loss = 0.00044903785757491587\n",
      "iters = 5200  loss = 0.00042755731300717493\n",
      "iters = 5300  loss = 0.0004088795695046307\n",
      "iters = 5400  loss = 0.00039239299147640383\n",
      "iters = 5500  loss = 0.0003776472827062378\n",
      "iters = 5600  loss = 0.000364308971136249\n",
      "iters = 5700  loss = 0.0003521316282408948\n",
      "iters = 5800  loss = 0.00034093099276337463\n",
      "iters = 5900  loss = 0.00033056625102282055\n",
      "iters = 6000  loss = 0.00032092712152650846\n",
      "A1 mean: 0.3162369330591486 A1 std: 0.6580523438173761\n",
      "dW 1 <e-8: 55468 total: 122880\n",
      "A2 mean: 0.21152925895308008 A2 std: 0.7908777133642327\n",
      "dW 2 <e-8: 17 total: 30\n",
      "A3 mean: 0.9561935246749669 A3 std: 0.012941294865057018\n",
      "dW 3 <e-8: 4 total: 6\n",
      "A4 mean: 0.020606319474689987 A4 std: 0.0\n",
      "dW 4 <e-8: 0 total: 2\n",
      "iters = 6100  loss = 0.000311925230270708\n",
      "iters = 6200  loss = 0.00030348833821179107\n",
      "iters = 6300  loss = 0.00029555640103775903\n",
      "iters = 6400  loss = 0.0002880788083606842\n",
      "iters = 6500  loss = 0.0002810123966403261\n",
      "iters = 6600  loss = 0.0002743199828969619\n",
      "iters = 6700  loss = 0.0002679692585147702\n",
      "iters = 6800  loss = 0.00026193193844943505\n",
      "iters = 6900  loss = 0.0002561830958128069\n",
      "iters = 7000  loss = 0.0002507006337847087\n",
      "iters = 7100  loss = 0.000245464861102693\n",
      "iters = 7200  loss = 0.00024045814692255667\n",
      "iters = 7300  loss = 0.00023566463735925593\n",
      "iters = 7400  loss = 0.00023107002056384001\n",
      "iters = 7500  loss = 0.0002266613304238901\n",
      "iters = 7600  loss = 0.00022242678131231865\n",
      "iters = 7700  loss = 0.00021835562802563816\n",
      "iters = 7800  loss = 0.00021443804633078855\n",
      "iters = 7900  loss = 0.00021066503050300606\n",
      "iters = 8000  loss = 0.00020702830497192016\n",
      "A1 mean: 0.3058134069945092 A1 std: 0.6585632553262668\n",
      "dW 1 <e-8: 60312 total: 122880\n",
      "A2 mean: 0.21634048868740438 A2 std: 0.8015968096165004\n",
      "dW 2 <e-8: 15 total: 30\n",
      "A3 mean: 0.9615287913692303 A3 std: 0.011097744582516169\n",
      "dW 3 <e-8: 4 total: 6\n",
      "A4 mean: 0.016901699145240968 A4 std: 0.0\n",
      "dW 4 <e-8: 0 total: 2\n",
      "iters = 8100  loss = 0.00020352024775918966\n",
      "iters = 8200  loss = 0.0002001338238314688\n",
      "iters = 8300  loss = 0.00019686252683822318\n",
      "iters = 8400  loss = 0.00019370032797758966\n",
      "iters = 8500  loss = 0.00019064163095170852\n",
      "iters = 8600  loss = 0.00018768123214827983\n",
      "iters = 8700  loss = 0.00018481428532691162\n",
      "iters = 8800  loss = 0.00018203627020417886\n",
      "iters = 8900  loss = 0.0001793429644258027\n",
      "iters = 9000  loss = 0.00017673041849211842\n",
      "iters = 9100  loss = 0.0001741949332673499\n",
      "iters = 9200  loss = 0.00017173303975675155\n",
      "iters = 9300  loss = 0.00016934148088039238\n",
      "iters = 9400  loss = 0.00016701719500989443\n",
      "iters = 9500  loss = 0.00016475730106611444\n",
      "iters = 9600  loss = 0.00016255908500250197\n",
      "iters = 9700  loss = 0.0001604199875216469\n",
      "iters = 9800  loss = 0.00015833759289192426\n",
      "iters = 9900  loss = 0.00015630961874774543\n",
      "iters = 10000  loss = 0.00015433390677117262\n",
      "A1 mean: 0.30550061052401306 A1 std: 0.659462733544249\n",
      "dW 1 <e-8: 62390 total: 122880\n",
      "A2 mean: 0.21863803169322346 A2 std: 0.8069591473275194\n",
      "dW 2 <e-8: 15 total: 30\n",
      "A3 mean: 0.9644738761329217 A3 std: 0.010021364152299761\n",
      "dW 3 <e-8: 4 total: 6\n",
      "A4 mean: 0.014705443878462045 A4 std: 0.0\n",
      "dW 4 <e-8: 0 total: 2\n",
      "iters = 10100  loss = 0.0001524084141649383\n",
      "iters = 10200  loss = 0.00015053120583750386\n",
      "iters = 10300  loss = 0.0001487004472299855\n",
      "iters = 10400  loss = 0.00014691439772274715\n",
      "iters = 10500  loss = 0.0001451714045664237\n",
      "iters = 10600  loss = 0.00014346989728819808\n",
      "iters = 10700  loss = 0.00014180838252947795\n",
      "iters = 10800  loss = 0.0001401854392757708\n",
      "iters = 10900  loss = 0.00013859971444368682\n",
      "iters = 11000  loss = 0.00013704991879356657\n",
      "iters = 11100  loss = 0.00013553482313947876\n",
      "iters = 11200  loss = 0.00013405325483112216\n",
      "iters = 11300  loss = 0.00013260409448468893\n",
      "iters = 11400  loss = 0.00013118627294197044\n",
      "iters = 11500  loss = 0.00012979876843897825\n",
      "iters = 11600  loss = 0.00012844060396710448\n",
      "iters = 11700  loss = 0.00012711084481144703\n",
      "iters = 11800  loss = 0.00012580859625231807\n",
      "iters = 11900  loss = 0.00012453300141723847\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.72\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    # File对象类似字典\n",
    "    train_data = h5py.File('./dataset/train_catvnoncat.h5', 'r')\n",
    "    test_data = h5py.File('./dataset/test_catvnoncat.h5', 'r')\n",
    "\n",
    "    train_x = np.array(train_data['train_set_x'][:])\n",
    "    train_y = np.array(train_data['train_set_y'][:])\n",
    "\n",
    "    test_x = np.array(test_data['test_set_x'][:])\n",
    "    test_y = np.array(test_data['test_set_y'][:])\n",
    "\n",
    "    # 还原图片\n",
    "    # plt.figure(figsize=(2, 2))\n",
    "    # plt.imshow(train_x[10])\n",
    "    # plt.show()\n",
    "\n",
    "    # reshape数据集\n",
    "    # 特征：64 * 64 * 3 = 12288\n",
    "    # (209, 64, 64, 3) -> (12288, 209)\n",
    "    train_x = train_x.reshape(train_x.shape[0], -1).T\n",
    "    test_x = test_x.reshape(test_x.shape[0], -1).T\n",
    "\n",
    "    # (209, ) -> (1, 209)\n",
    "    train_y = train_y.reshape(1, train_y.shape[0])\n",
    "    test_y = test_y.reshape(1, test_y.shape[0])\n",
    "    \n",
    "    return train_x, train_y, test_x, test_y\n",
    "\n",
    "\n",
    "def init_params(layers, activation='sigmoid'):\n",
    "    # 根据传入的数组初始化每一层的W和b\n",
    "    params = {}\n",
    "    layer_num = len(layers)\n",
    "    \n",
    "    for n in range(1, layer_num):\n",
    "        if activation == 'tanh':\n",
    "            # Xavier初始化，使W的方差等于上一层神经元个数的倒数\n",
    "            params['W'+str(n)] = np.random.randn(layers[n], layers[n-1]) * np.sqrt(1 / layers[n-1])\n",
    "        elif activation == 'ReLU':\n",
    "            # He初始化，ReLU激活函数的解决方案\n",
    "            params['W'+str(n)] = np.random.randn(layers[n], layers[n-1]) * np.sqrt(2 / layers[n-1])\n",
    "        else:\n",
    "            params['W'+str(n)] = np.random.randn(layers[n], layers[n-1]) * 0.01  # 标准正态分布，*0.01将数据保持在±0.03之间\n",
    "        params['b'+str(n)] = np.zeros((layers[n], 1))\n",
    "    \n",
    "    for n in range(1, layer_num):\n",
    "        print('W'+str(n)+'=', params['W'+str(n)].shape)\n",
    "        print('b'+str(n)+'=', params['b'+str(n)].shape)\n",
    "    \n",
    "    return params\n",
    "\n",
    "\n",
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "\n",
    "def ReLU(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "\n",
    "def tanh(Z):\n",
    "    return np.tanh(Z)\n",
    "    \n",
    "    \n",
    "def sigmoid_grad(A):\n",
    "    # s'(x) = s(x) * (1 - s(x))，所以此处是f(Z)而非Z\n",
    "    return A * (1 - A)\n",
    "\n",
    "\n",
    "def tanh_grad(A):\n",
    "    return 1 - np.square(A)\n",
    "\n",
    "\n",
    "def ReLU_grad(Z):\n",
    "    return np.array(Z > 0)\n",
    "    \n",
    "    \n",
    "def forward_prop(X, params, activation='sigmoid'):\n",
    "    # 每一层WX+b的结果Z和激活后的结果A，用于反向传播\n",
    "    # 要放入原始数据，即A0，否则反向传播会出问题\n",
    "    cache = {'A0': X}  \n",
    "    A = X\n",
    "    layer_num = len(params) // 2  # 用/会得到浮点数\n",
    "    for n in range(1, layer_num):\n",
    "        # b被broadcast，（Ln, 1) -> (Ln, m), m为传入的样本数量\n",
    "        Z = np.dot(params['W'+str(n)], A) + params['b'+str(n)]\n",
    "        A = eval(activation+'(Z)')\n",
    "        cache['Z'+str(n)] = Z\n",
    "        cache['A'+str(n)] = A\n",
    "    \n",
    "    # 最后一层使用sigmoid\n",
    "    Z = np.dot(params['W'+str(layer_num)], A) + params['b'+str(layer_num)]\n",
    "    A = sigmoid(Z)\n",
    "    cache['Z'+str(layer_num)] = Z\n",
    "    cache['A'+str(layer_num)] = A\n",
    "    \n",
    "    return A, cache\n",
    "    \n",
    "    \n",
    "def mean_square_loss(A_last, Y):\n",
    "    m = Y.shape[1]\n",
    "    return np.sum((A_last - Y) * (A_last - Y)) / m / 2\n",
    "    \n",
    "\n",
    "def back_prop(Y, params, cache, activation='sigmoid'):\n",
    "    layer_num = len(params) // 2\n",
    "    m = Y.shape[1]\n",
    "    grads = {}  # 保存所有梯度\n",
    "    \n",
    "    # 最后一层的Z和A\n",
    "    A_last = cache['A'+str(layer_num)]\n",
    "    Z_last = cache['Z'+str(layer_num)]\n",
    "    \n",
    "    # 数组的形状\n",
    "    # W(N(L), N(L-1)), 此处W是当前层左侧边权重，这样Z(L) = W(L)A(L-1)+b(L)不会矛盾\n",
    "    # X/A/Z(N(L), m), b(N(L), 1)\n",
    "    \n",
    "    # 公式\n",
    "    # (最后一层) dZ = (A(L) - y) * f'(Z(L)) /// (N(L), m), (N(L), m--广播后)\n",
    "    # (非最后一层) dZ = (W(L+1).T) dZ(L+1) * f'(Z(L)) /// (N(L), N(L+1)), (N(L+1), m), (N(L), m--广播后)\n",
    "    # 此处W(L+1)是指Z所在神经元右侧的W\n",
    "    # dW = (1/m) * (dJ/dZ(L)) (A(L-1).T) /// (N(L), m), (m, N(L-1))\n",
    "    # db = (1/m) * sum(dJ/dZ(L)) /// (N(L), m) -> (N(L), 1)\n",
    "    \n",
    "    # 最后一层（输出层）dJ/dZ，且使用sigmoid\n",
    "    dZ = (A_last - Y) * sigmoid_grad(A_last)\n",
    "    # 最后一层dJ/dW和dJ/db\n",
    "    grads['dW'+str(layer_num)] = np.dot(dZ, cache['A'+str(layer_num-1)].T) / m\n",
    "    # 在第二维求和（横向），保持二维\n",
    "    grads['db'+str(layer_num)] = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "    \n",
    "    for n in range(layer_num-1, 0, -1):\n",
    "        if activation == 'ReLU':\n",
    "            dZ = np.dot(params['W'+str(n+1)].T, dZ) * eval(activation + '_grad' + \"(cache['Z'+str(n)])\")\n",
    "        else:\n",
    "            dZ = np.dot(params['W'+str(n+1)].T, dZ) * eval(activation + '_grad' + \"(cache['A'+str(n)])\")\n",
    "        grads['dW'+str(n)] = np.dot(dZ, cache['A'+str(n-1)].T) / m\n",
    "        grads['db'+str(n)] = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "    \n",
    "    return grads\n",
    "        \n",
    "\n",
    "def gradient_descent(grads, params, lr):\n",
    "    # W = W - lr * dW，b相同\n",
    "    layer_num = len(params) // 2\n",
    "    for n in range(1, layer_num+1):\n",
    "        params['W'+str(n)] = params['W'+str(n)] - lr * grads['dW'+str(n)]\n",
    "        params['b'+str(n)] = params['b'+str(n)] - lr * grads['db'+str(n)]\n",
    "        \n",
    "    return params\n",
    "\n",
    "\n",
    "def dict_to_col_vector(grads_or_params, if_grads):\n",
    "    # 将梯度或参数字典转为列向量\n",
    "    layer_num = len(grads_or_params) // 2\n",
    "    prefix = ''\n",
    "    if if_grads == True:\n",
    "        prefix = 'd'\n",
    "    \n",
    "    col_vec = np.array([[0]]) # 为了方便连接\n",
    "    for n in range(1, layer_num+1):\n",
    "        dW_vec = np.reshape(grads_or_params[prefix+'W'+str(n)], (-1, 1))\n",
    "        db_vec = np.reshape(grads_or_params[prefix+'b'+str(n)], (-1, 1))\n",
    "        col_vec = np.concatenate((col_vec, dW_vec, db_vec), axis=0)\n",
    "    \n",
    "    return col_vec[1:]\n",
    "\n",
    "\n",
    "def col_vector_to_dict(col_vec, params_src):\n",
    "    # 根据原始字典params_src的形状将col_vec复原\n",
    "    layer_num = len(params_src) // 2\n",
    "    reshaped = {}\n",
    "    \n",
    "    idx_start = 0\n",
    "    idx_end = 0\n",
    "    for n in range(1, layer_num+1):\n",
    "        # W\n",
    "        row = params_src['W'+str(n)].shape[0]\n",
    "        col = params_src['W'+str(n)].shape[1]\n",
    "        idx_end = idx_start + row * col\n",
    "        reshaped['W'+str(n)] = col_vec[idx_start:idx_end].reshape((row, col))\n",
    "        idx_start = idx_end\n",
    "        # b\n",
    "        row = params_src['b'+str(n)].shape[0]\n",
    "        col = params_src['b'+str(n)].shape[1]\n",
    "        idx_end = idx_start + row * col\n",
    "        reshaped['b'+str(n)] = col_vec[idx_start:idx_end].reshape((row, col))\n",
    "        idx_start = idx_end\n",
    "    \n",
    "    return reshaped\n",
    "\n",
    "\n",
    "def L2(vec):\n",
    "    return np.sqrt(np.sum(np.square(vec)))\n",
    "\n",
    "\n",
    "def gradient_check(X, Y, grads, params, epsilon=1e-6, activation='sigmoid'):\n",
    "    grads_vec = dict_to_col_vector(grads, True) # 使用导数定义求得的梯度值\n",
    "    params_vec = dict_to_col_vector(params, False)\n",
    "    grads_check = np.zeros(grads_vec.shape)\n",
    "    params_num = params_vec.shape[0]\n",
    "    \n",
    "    # 整体的梯度检验，也可以对每个W和b进行单独检验\n",
    "    for i in range(params_num):\n",
    "        if i % 1000 == 0:\n",
    "            print('checking gradient for parameters, checked:', i)\n",
    "        # 对 参数+epsilon 求损失J\n",
    "        params_vec[i][0] = params_vec[i][0] + epsilon\n",
    "        A_last, _ = forward_prop(X, col_vector_to_dict(params_vec, params), activation)\n",
    "        J_plus_e = mean_square_loss(A_last, Y)\n",
    "        # 对 参数-epsilon 求损失J\n",
    "        params_vec[i][0] = params_vec[i][0] - 2 * epsilon  # 相当于原参数减去epsilon\n",
    "        A_last, _ = forward_prop(X, col_vector_to_dict(params_vec, params), activation)\n",
    "        J_minus_e = mean_square_loss(A_last, Y)\n",
    "        # 复原params_vec\n",
    "        params_vec[i][0] = params_vec[i][0] + epsilon\n",
    "        \n",
    "        grads_check[i][0] = (J_plus_e - J_minus_e) / epsilon / 2\n",
    "    \n",
    "#         curr_grad = (J_plus_e - J_minus_e) / epsilon / 2\n",
    "#         diff = abs(grads_vec[i][0] - curr_grad) / max(1, abs(grads_vec[i][0]), abs(curr_grad))\n",
    "#         if diff > 1e-4:\n",
    "#             print(\"Gradient check failed, first error index %s.\" % str(i))\n",
    "#             print(\"The diff is\", diff)\n",
    "#             print(\"Your gradient: %.9f \\t Numerical gradient: %.9f\" % (grads_vec[i][0], curr_grad))\n",
    "#             return\n",
    "    diff = L2(grads_check - grads_vec) / (L2(grads_check) + L2(grads_vec))\n",
    "    print(\"The diff is\", diff)\n",
    "\n",
    "\n",
    "def train(layers, train_x, train_y, grads_check=False, iters=12000, lr=0.1, activation='sigmoid'):\n",
    "    # 初始化w和b\n",
    "    params = init_params(layers, activation)\n",
    "    losses = []\n",
    "    for i in range(iters):\n",
    "        # 前向传播, A_last(1, 209)\n",
    "        A_last, cache = forward_prop(train_x, params, activation)\n",
    "\n",
    "        # 计算loss\n",
    "        loss = mean_square_loss(A_last, train_y)\n",
    "        if i % 100 == 0:\n",
    "            print('iters =', i, ' loss =', loss)\n",
    "            losses.append(loss)\n",
    "\n",
    "        # 反向传播\n",
    "        grads = back_prop(train_y, params, cache, activation)\n",
    "        if grads_check and i == iters - 1:\n",
    "            gradient_check(train_x, train_y, grads, params, activation)\n",
    "\n",
    "        # 梯度下降更新参数\n",
    "        params = gradient_descent(grads, params, lr)\n",
    "    \n",
    "        # 检查梯度消失问题\n",
    "        if i > 0 and i % 2000 == 0:\n",
    "            for n in range(1, len(layers)):\n",
    "                # 激活值的均值和标准差\n",
    "                print('A' + str(n) + ' mean:', np.mean(cache['A'+str(n)][:,1]), 'A' + str(n) + ' std:', np.std(cache['A'+str(n)][:,1]))\n",
    "                # 梯度接近0的参数统计\n",
    "                print('dW ' + str(n) + ' <e-8:', np.sum(grads['dW'+str(n)] < 1e-8), 'total:', grads['dW' + str(n)].shape[0] * grads['dW' + str(n)].shape[1])\n",
    "    \n",
    "    plt.plot(losses, 'r')\n",
    "    plt.xlabel('iters')\n",
    "    plt.ylabel('loss')\n",
    "    plt.show()\n",
    "    \n",
    "    return params\n",
    "\n",
    "\n",
    "def predict(X, Y, params, activation='sigmoid'):\n",
    "    m = X.shape[1] # 样本数量\n",
    "    A_last, _ = forward_prop(X, params, activation)\n",
    "    result = np.zeros(A_last.shape)\n",
    "    for i in range(A_last.shape[1]):\n",
    "        if A_last[0][i] >= 0.5:\n",
    "            result[0][i] = 1\n",
    "        else:\n",
    "            result[0][i] = 0\n",
    "    acc = np.sum(result==Y) / m\n",
    "    print('accuracy =', acc)\n",
    "    \n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    train_x, train_y, test_x, test_y = load_data()\n",
    "    # 归一化\n",
    "    train_x = train_x / 255.0\n",
    "    test_x = test_x / 255.0\n",
    "    \n",
    "    layers = [12288, 10, 3, 2, 1]\n",
    "#     layers = [12288, 10, 1]\n",
    "    params = train(layers, train_x, train_y, activation='tanh')\n",
    "    predict(test_x, test_y, params, activation='tanh')\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
