{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1= (10, 12288)\n",
      "b1= (10, 1)\n",
      "W2= (3, 10)\n",
      "b2= (3, 1)\n",
      "W3= (2, 3)\n",
      "b3= (2, 1)\n",
      "W4= (1, 2)\n",
      "b4= (1, 1)\n",
      "epoches = 0  loss = 0.1165878725802911\n",
      "epoches = 500  loss = 0.05010292470027547\n",
      "epoches = 1000  loss = 0.024857122691598232\n",
      "epoches = 1500  loss = 0.004596168564993375\n",
      "epoches = 2000  loss = 0.0020703911944966184\n",
      "epoches = 2500  loss = 0.0122294769186951\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deXhV1fn28e8jkziAitEqoEGNVlSqNOA8gQNQC61iRavi0KJVnCdwQkBBnP3VoaVSh1JF1GrxhYq1KlW0SHBGhCJOSFsiIOBQEHzeP9ZOifEEknB21hnuz3Xl4uScfZL7dOBm77XXWubuiIiI1LRB7AAiIpKbVBAiIpKRCkJERDJSQYiISEYqCBERyahp7ADZsuWWW3ppaWnsGCIieWXGjBmfuntJptdSLQgz6wHcDjQB7nH362u8fhBwG9AJ6OfujybP7wncDbQCVgPXufvDa/tdpaWlVFRUZP9DiIgUMDP7sLbXUrvEZGZNgDuBnkBH4Hgz61jjsI+AU4AHazz/JXCyu+8G9ABuM7PN0soqIiLfleYZRFdgrrvPAzCzcUAf4J2qA9z9g+S1b6q/0d3nVHu8wMwWAiXAZynmFRGRatIcpG4LfFzt+/nJc/ViZl2B5sB7GV4bYGYVZlZRWVnZ4KAiIvJdaRaEZXiuXut6mNk2wB+AU939m5qvu/tody939/KSkoxjLCIi0kBpFsR8oH2179sBC+r6ZjNrBUwErnT3f2Q5m4iIrEOaBTEdKDOzDmbWHOgHTKjLG5PjHwcecPdHUswoIiK1SK0g3H0VMBCYDMwCxrv7TDMbZma9Acysi5nNB44FfmtmM5O3/ww4CDjFzF5PvvZMK6uIiHyXFcpy3+Xl5d6geRDLlsGoUXDKKVBWlvVcIiK5zMxmuHt5ptcKZiZ1g335Jdx2G8ybBw89FDuNiEjO0FpM3/senH8+jBsHr70WO42ISM5QQQBccglsvjlcfnnsJCIiOUMFAbDZZjB4MDz1FEyZEjuNiEhOUEFUGTgQtt02FEWBDNyLiKwPFUSVli1hyBB4+WV48snYaUREolNBVHfqqeFW18svh9WrY6cREYlKBVFds2Zw7bUwcyY8WHMFchGR4qKCqKlvX+jcGa6+GlasiJ1GRCQaFURNG2wAI0bABx/A6NGx04iIRKOCyOSII+CQQ2D4cFi+PHYaEZEoVBCZmMHIkVBZGZbhEBEpQiqI2uyzD/zkJ3DTTfDpp7HTiIg0OhXE2lx7LXz+OVx/fewkIiKNTgWxNrvtBiedBHfcAR9/vO7jRUQKiApiXa65Jiy9MXRo7CQiIo1KBbEupaXwq1/BvffCu+/GTiMi0mhUEHVx+eWw0UZw1VWxk4iINBoVRF1stRVcdBE8+ihMnx47jYhIo1BB1NWFF0KbNtpUSESKhgqirlq1giuugGeegb/9LXYaEZHUqSDq41e/gvbttamQiBQFFUR9bLhhuN11+nR4/PHYaUREUqWCqK+TToJddw2Xm1atip1GRCQ1Koj6ato0LMHx7rvwwAOx04iIpEYF0RA//Sl06RL2sP7vf2OnERFJRaoFYWY9zGy2mc01s0EZXj/IzF41s1Vm1rfGa/3N7J/JV/80c9abWVjAb/58uPvu2GlERFKRWkGYWRPgTqAn0BE43sw61jjsI+AU4MEa790CGALsDXQFhpjZ5mllbZBu3eDww+G662DZsthpRESyLs0ziK7AXHef5+4rgXFAn+oHuPsH7v4m8E2N9x4J/NXdF7v7EuCvQI8UszbMiBGwaBHcfHPsJCIiWZdmQbQFqq+RPT95LmvvNbMBZlZhZhWVlZUNDtpg5eXQt28oiIULG//3i4ikKM2CsAzP1XV2WZ3e6+6j3b3c3ctLSkrqFS5rrr02DFRfd12c3y8ikpI0C2I+0L7a9+2ABY3w3sa1yy5w6qnwm9/ABx/ETiMikjVpFsR0oMzMOphZc6AfMKGO750MHGFmmyeD00ckz+WmIUPCnU3XXBM7iYhI1qRWEO6+ChhI+It9FjDe3Wea2TAz6w1gZl3MbD5wLPBbM5uZvHcxMJxQMtOBYclzualdOzjnnDBx7u23Y6cREckK8wJZdK68vNwrKiriBVi0CHbYAQ49FJ54Il4OEZF6MLMZ7l6e6TXNpM6WNm3gkkvgz3+Gl1+OnUZEZL2pILLp/PPD7nNaDlxECoAKIps22STsWz1lCjz9dOw0IiLrRQWRbQMGQGlpOIv4puYEcRGR/KGCyLbmzWHYMHjtNXjkkdhpREQaTAWRhhNOgN13D5ebvv46dhoRkQZRQaShSZOwkN8//wn33hs7jYhIg6gg0nLUUbDffmEP6y+/jJ1GRKTeVBBpqdpUaMECuOOO2GlEROpNBZGmAw+Enj1DUXz2Wew0IiL1ooJI24gRsGQJ3Hhj7CQiIvWigkjbnnvC8cfDbbfBv/4VO42ISJ2pIBrDsGGwcmXYXEhEJE+oIBrDTjvBL38Jo0fDe+/FTiMiUicqiMZy1VXQrBlcfXXsJCIidaKCaCzbbAPnnQcPPQRvvBE7jYjIOqkgGtOll0Lr1nDFFbGTiIiskwqiMW2+OQwaBBMnwgsvxE4jIrJWKojGds454XKTNhUSkRyngmhsG20UBqqnToVJk2KnERGplQoihtNPhx131KZCIpLTVBAxNGsWJs299Va4q0lEJAepIGL52c/CMhxXXRVmWYuI5BgVRCwbbAAjR8L778Pvfhc7jYjId6ggYjrySDjoIBg+HL74InYaEZFvSbUgzKyHmc02s7lmNijD6y3M7OHk9WlmVpo838zM7jezt8xslpkNTjNnNGbhLOI//4Hbb4+dRkTkW1IrCDNrAtwJ9AQ6AsebWccah50OLHH3nYBbgVHJ88cCLdx9D+CHwBlV5VFw9tsPeveGUaNg0aLYaURE/ifNM4iuwFx3n+fuK4FxQJ8ax/QB7k8ePwp0NzMDHNjYzJoCLYGVwLIUs8Z13XWwfHkoCRGRHJFmQbQFPq72/fzkuYzHuPsqYCnQhlAWXwD/Aj4CbnL3xTV/gZkNMLMKM6uorKzM/idoLLvvDiedBL/+NXzySew0IiJAugVhGZ6rubZEbcd0BVYD2wIdgIvMbIfvHOg+2t3L3b28pKRkffPGNXQorF4dNhcSEckBaRbEfKB9te/bAQtqOya5nNQaWAycADzl7l+7+0JgKlCeYtb4SkvhzDNhzBiYMyd2GhGRVAtiOlBmZh3MrDnQD5hQ45gJQP/kcV/gWXd3wmWlbhZsDOwDvJti1txwxRWw4YZh8pyISGSpFUQypjAQmAzMAsa7+0wzG2ZmvZPDxgBtzGwucCFQdSvsncAmwNuEornX3d9MK2vO2HpruPBCGD8eXn01dhoRKXLmBbLkdHl5uVdUVMSOsf6WLg0L+ZWXw1NPxU4jIgXOzGa4e8ZL+JpJnWtatw6rvE6eDM89FzuNiBQxFUQuOussaNdOmwqJSFQqiFzUsiVccw1MmwZ//nPsNCJSpFQQuap/f9hll3Bn0+rVsdOISBFSQeSqpk3DEhzvvANjx8ZOIyJFSAWRy44+OtzNdPXVsGJF7DQiUmRUELmsajnwjz6C3/wmdhoRKTIqiFx32GHQvXvYw3r58thpRKSIqCDywciR8OmncMstsZOISBFRQeSDLl3gmGPg5pshn5c1F5G8ooLIF1X7Vo8cGTuJiBQJFUS+2HVXOOUUuPPOMGgtIpIyFUQ+GTIk3Nl0zTWxk4hIEVBB5JPttoOzz4b774dZs2KnEZECp4LIN4MHw8Ybw5VXxk4iIgVOBZFvttwSLr4Y/vQneOWV2GlEpICpIPLRBRdASQkMGqTlwEUkNSqIfLTppuES03PPwTPPxE4jIgVKBZGvzjgDtt9emwqJSGpUEPmqRQsYOhRmzIDHHoudRkQKkAoin514InTsGDYVWrUqdhoRKTAqiHzWpAmMGAFz5sB998VOIyIFRgWR73r3hn32CbOrv/oqdhoRKSAqiHxnBtdfD598AnfdFTuNiBSQOhWEmZ1nZq0sGGNmr5rZEWmHkzo6+GA48shwuWnp0thpRKRA1PUM4jR3XwYcAZQApwLXp5ZK6m/ECFi8GG66KXYSESkQdS0IS/7sBdzr7m9Ue672N5n1MLPZZjbXzAZleL2FmT2cvD7NzEqrvdbJzF42s5lm9paZbVjHrMWpc2c47ji49Vb4z39ipxGRAlDXgphhZk8TCmKymW0KfLO2N5hZE+BOoCfQETjezDrWOOx0YIm77wTcCoxK3tsUGAuc6e67AYcAX9cxa/EaPhz++1+47rrYSUSkANS1IE4HBgFd3P1LoBnhMtPadAXmuvs8d18JjAP61DimD3B/8vhRoLuZGeFS1pvJmQruvsjdV9cxa/EqK4Nf/AJ+8xt4//3YaUQkz9W1IPYFZrv7Z2Z2InAlsK7R0LbAx9W+n588l/EYd1+V/Mw2wM6Am9nkZED80ky/wMwGmFmFmVVUaq/m4KqrwvyIIUNiJxGRPFfXgrgb+NLMfgBcCnwIPLCO92Qao6i5aFBtxzQFDgB+nvz5UzPr/p0D3Ue7e7m7l5eUlKwjTpFo2xbOPRfGjoW33oqdRkTyWF0LYpW7O+GS0O3ufjuw6TreMx9oX+37dsCC2o5Jxh1aA4uT56e4+6fJJa1JQOc6ZpXLLoNWrcISHCIiDVTXglhuZoOBk4CJyQB0s3W8ZzpQZmYdzKw50A+YUOOYCUD/5HFf4NmkiCYDncxso6Q4DgbeqWNW2WKLUBJPPgkvvRQ7jYjkqboWxHHACsJ8iH8Txg5uXNsbkjGFgYS/7GcB4919ppkNM7PeyWFjgDZmNhe4kDAQjrsvAW4hlMzrwKvuPrFen6zYnXsufO972lRIRBrMvI5/eZjZ1kCX5NtX3H1haqkaoLy83CsqKmLHyC133QVnnw2TJkHPnrHTiEgOMrMZ7l6e6bW6LrXxM+AV4FjgZ8A0M+ubvYiSil/8AnbYIWwq9M1ap62IiHxHXS8xXUGYA9Hf3U8mzHG4Kr1YkhXNm4fJc2+8AQ8/HDuNiOSZuhbEBjUuKS2qx3slpn79oFOnMD/ia01GF5G6q+tf8k8lk9ZOMbNTgImEW08l122wAYwcCe+9B2PGxE4jInmkPoPUxwD7Eya3/d3dH08zWH1pkHot3OGgg2Du3FAUG20UO5GI5Ij1HqQGcPfH3P1Cd78g18pB1sEsnEX8+9/wf/8XO42I5Im1FoSZLTezZRm+lpvZssYKKVlwwAFw1FEwahQsWRI7jYjkgbUWhLtv6u6tMnxt6u6tGiukZMl114Ud5264IXYSEckDuhOpmHTqBD//Odx+OyyouSyWiMi3qSCKzdCh4XbX4cNjJxGRHKeCKDY77ABnnAG/+x08/3zsNCKSw1QQxejqq0NRdO8OV16pCXQikpEKohhttRW8+ir07x8Grg88MMyPEBGpRgVRrDbZBH7/exg/HmbPhj33hAce0NLgIvI/Kohid+yxYTG/zp3DGcUJJ8Bnn8VOJSI5QAUhsN128OyzcO218Mgj4WzixRdjpxKRyFQQEjRpEvawnjo1PD74YBgyBFatip1MRCJRQci37b03vPYanHgiDBsWFvl7//3YqUQkAhWEfFerVnD//fDQQ/DOO/CDH8Af/xg7lYg0MhWE1K5fvzCA3alTOKM48cSwlpOIFAUVhKzd9tuHGddDh8K4cWEA++WXY6cSkUaggpB1a9o0zL5+4YWwt8SBB4bxCQ1gixQ0FYTU3b77wuuvw/HHhzucDjkEPvggdioRSYkKQuqnVSv4wx9g7Fh4880wgD1uXOxUIpICFYQ0zM9/Hgawd9stnFH07w/Ll8dOJSJZlGpBmFkPM5ttZnPNbFCG11uY2cPJ69PMrLTG69uZ2edmdnGaOaWBOnSAv/89jE+MHRsGsKdNi51KRLIktYIwsybAnUBPoCNwvJl1rHHY6cASd98JuBUYVeP1W4G/pJVRsqBp03CH05QpsHo17L9/WCF29erYyURkPaV5BtEVmOvu89x9JTAO6FPjmD7A/cnjR4HuZmYAZvYTYB4wM8WMki0HHBAGsI89Nuwx0a0bfPRR7FQish7SLIi2wMfVvp+fPJfxGHdfBSwF2pjZxsBlwNC1/QIzG2BmFWZWUVlZmbXg0kCbbQYPPhhmYb/6ahjAHj8+dioRaaA0C8IyPFdzs4HajhkK3Orun6/tF7j7aHcvd/fykpKSBsaUrDKDk08OZxO77ALHHQennQafr/W/ShHJQWkWxHygfbXv2wELajvGzJoCrYHFwN7ADWb2AXA+cLmZDUwxq2TbjjuGiXVXXgn33Qd77QXTp8dOJSL1kGZBTAfKzKyDmTUH+gETahwzAeifPO4LPOvBge5e6u6lwG3ACHe/I8WskoZmzWD48LBUx4oVsN9+cP31GsAWyROpFUQypjAQmAzMAsa7+0wzG2ZmvZPDxhDGHOYCFwLfuRVWCsBBB4U5E0cfDYMHw2GHwfz5sVOJyDqYF8gexOXl5V5RURE7hqyNe7jcdM450Lw5/O53cMwxsVOJFDUzm+Hu5Zle00xqaTxmcOqpYUOinXaCvn3hl7+EL76InUxEMlBBSOMrKwtbmw4eDGPGQOfOMGNG7FQiUoMKQuJo1gxGjIBnnw1nEPvuCzfcAN98EzuZiCRUEBLXIYeEVWF794bLLoMjjoBPPomdSkRQQUgu2GILeOQRuOeesFtdp07wxBOxU4kUPRWE5AYzOP30MIDdoQP89KdwxhkawBaJSAUhuWXnneGll+DSS8NtsOXloTREpNGpICT3NG8Oo0bBM8/AsmWw995w880awBZpZCoIyV3duoUB7KOOgosvhh494F//ip1KpGioICS3tWkDjz0Gv/0tvPhiGMB+8snYqUSKggpCcp8ZDBgQ9pho3z7cEnvWWfDll7GTiRQ0FYTkj+9/P9wGe9FFcPfd0KVLuAQlIqlQQUh+adECbroJnn4aliwJJXHbbRrAFkmBCkLy0+GHh7OHHj3gggugVy/4979jpxIpKCoIyV9bbhlmXN99N0yZEgawJ06MnUqkYKggJL+ZwZlnhtVgt9023BJ7zjnw1Vexk4nkPRWEFIaOHWHatHC56Y47oGtXePvt2KlE8poKQgpHixZwyy3w1FNQWRmW6fj1r8NOdiJSbyoIKTxHHhkGsA87DM49N1x2WrgwdiqRvKOCkMK01VZhxvUdd8Df/gZ77AF/+UvsVCJ5RQUhhcsMzj4bKipCYfTqBccdBx9+GDuZSF5QQUjh2313mD4dhgwJZxXf/z5cdZX2mhBZBxWEFIcNN4RrroHZs8NmRNdeG/aeGDtWs7BFaqGCkOLSvj08+CBMnRrmTZx0Euy3X7hFVkS+RQUhxamqFO67L4xJ7LNPKItPPomdTCRnqCCkeG2wAfTvD3PmwODB8Mgj4bLT8OGaiS1CygVhZj3MbLaZzTWzQRleb2FmDyevTzOz0uT5w81shpm9lfzZLc2cUuQ23RRGjIBZs6BnT7j66jCQ/fDDmmQnRS21gjCzJsCdQE+gI3C8mXWscdjpwBJ33wm4FRiVPP8p8GN33wPoD/whrZwi/9OhAzz6KDz/PGy+OfTrBwcdFNZ5EilCaZ5BdAXmuvs8d18JjAP61DimD3B/8vhRoLuZmbu/5u4LkudnAhuaWYsUs4qscfDBoRRGjw53PXXpAqedpuXEpeikWRBtgY+rfT8/eS7jMe6+ClgKtKlxzDHAa+6+ouYvMLMBZlZhZhWVlZVZCy5Ckybwy1/CP/8ZdrAbOxbKymDUKFjxnf8pihSkNAvCMjxX84LuWo8xs90Il53OyPQL3H20u5e7e3lJSUmDg4rUqnVruPFGmDkTunWDQYPCyrGPP67xCSl4aRbEfKB9te/bAQtqO8bMmgKtgcXJ9+2Ax4GT3f29FHOKrFtZGfz5z2Gr05Yt4eijoXt37YktBS3NgpgOlJlZBzNrDvQDJtQ4ZgJhEBqgL/Csu7uZbQZMBAa7+9QUM4rUz+GHw+uvw513whtvwF57hQ2LdIlTClBqBZGMKQwEJgOzgPHuPtPMhplZ7+SwMUAbM5sLXAhU3Qo7ENgJuMrMXk++tkorq0i9NG0KZ50VxicGDoR77glnGLfcAitXxk4nkjXmBXIdtby83CsqKmLHkGI0axZceGHYqGjnneHmm+FHPwqryYrkODOb4e7lmV7TTGqR9bXrrmGviYkTQyn8+MfQowe8807sZCLrRQUhki29esFbb8Gtt8Irr0CnTnDOObB4cexkIg2ighDJpmbN4Pzzw/jEgAFw112w005hZ7tVq2KnE6kXFYRIGrbcMpTD669D587hTOIHPwi3yYrkCRWESJr22AP++ld44okwA/vII8MYxZw5sZOJrJMKQiRtZtCnT5iNfcMNMGVK2Ab1oovgs89ipxOplQpCpLG0aAGXXBLGJ04+OQxml5XBb38Lq1fHTif56Ouvw+rDTz6Zyo9XQYg0tq23DpPrZswIt8ieeWYYp3juudjJJB8sWABjxkDfvmGs69BDw4ZXKVBBiMSy117hctP48bB0aVgM8OijYd682Mkkl6xaBS++CFdcEf4307Yt/OIX8I9/wHHHhYUjX345lV+tmdQiueCrr8JSHSNHhssGF1wQ/kLYdNPYySSGhQvDzPxJk2Dy5DBW1aQJ7L9/mG/Tq1cYx8rCbP21zaRWQYjkkgULwuWCBx4Il6JGjIBTTgn7Z0vhWr0aKipCIUyaFB4DfO97YRvcXr3gsMNgs82y/qtVECL55pVXwoS7l18O4xO33w4HHBA7lWTTokXh7GDSpHC2sGhR+IfAPvuEQujZE/bcM/V/HKytIJqm+ptFpGG6doWpU+Ghh+Cyy+DAA8P15htugO22i51OGuKbb+C119acJUybFjad2nLLNYVwxBHQpuammvHoDEIk133xRSiGG24I319ySSiNjTeOm0vWbcmSMFGy6izhP/8J4wZduqwphfLyqJcQdYlJpBB89FHY8vShh8KdLNdfDyecoPGJXOIedhmcNCms8PvSS2F8YfPNwyz6Xr3Cn1vlzvY2KgiRQjJ1Kpx3XphHsffeYXxi771jpypey5bBM8+EQpg0KdxoAOGW1Ko7jrp2DRtN5SCNQYgUkv33D4PYDzwQ7njaZx848cRwRtG2bex0hc897PVRVQgvvBDmKrRqFcYQevUK+4Fss03spOtNZxAi+Wz58jB34pZbwn3ygwbBxRdDy5axkxWWL76AZ59dM8D80Ufh+T32WHOWsO++Ybn3PKNLTCKF7v33w+D1Y4/B9tuHAe1jj9W2pw3lHtbMqiqEKVPCfuObbBLmI1QNMLdrFzvpelNBiBSL558P8yfeeCPMmxg+HHbcMdw62bKlCmNtvvoq/OdXVQpVS57suuuas4QDDoDmzaPGzDaNQYgUi0MOCYPXv/99WKrj0EPXvNaiRSiKLbYIX3V9XMiXq+bNW1MIzz0H//1v+Lzdu4dLdT17Qmlp7JTR6AxCpFAtXQp/+1uYobt48Zo/Mz1esaL2n7PhhvUrlKrHuVgsK1bA3/++5jbU2bPD82Vlay4bHXxw+MxFQmcQIsWodeuwOuy6uIfLK+sqkarHc+aseW7lytp/bsuWay+R2ool2385f/hhKIO//CUU5hdfhLOpQw+Fs84KpVBWlt3fWSBUECLFzgw22ih81WfQ1R2+/HLdhVL1ePbs8HjRorBibW022qj+l8G22CL8pQ+htKZOXXMb6syZ4fnSUujfP5wpHHpo+D2yVioIEWkYs7Dcx8YbQ/v2dX9fVbHU5Wxl0SKYNWvN43UVS5s2YWns5cvDLacHHwynnRZKYZddNEhfT6kWhJn1AG4HmgD3uPv1NV5vATwA/BBYBBzn7h8krw0GTgdWA+e6++Q0s4pII6leLPVZeNA9XB5aV6G0bBmWs+jWTftprKfUCsLMmgB3AocD84HpZjbB3d+pdtjpwBJ338nM+gGjgOPMrCPQD9gN2BZ4xsx2dndt3CtSrMzCPIRNNtGKto0kzVW+ugJz3X2eu68ExgF9ahzTB7g/efwo0N3MLHl+nLuvcPf3gbnJzxMRkUaSZkG0BT6u9v385LmMx7j7KmAp0KaO78XMBphZhZlVVFZWZjG6iIikWRCZRoNqTrqo7Zi6vBd3H+3u5e5eXlJS0oCIIiJSmzQLYj5Q/daGdsCC2o4xs6ZAa2BxHd8rIiIpSrMgpgNlZtbBzJoTBp0n1DhmAtA/edwXeNbD1O4JQD8za2FmHYAy4JUUs4qISA2p3cXk7qvMbCAwmXCb6+/dfaaZDQMq3H0CMAb4g5nNJZw59EveO9PMxgPvAKuAs3UHk4hI49JaTCIiRWxtazFpM1sREcmoYM4gzKwS+HA9fsSWwKdZipMviu0zF9vnBX3mYrE+n3l7d894G2jBFMT6MrOK2k6zClWxfeZi+7ygz1ws0vrMusQkIiIZqSBERCQjFcQao2MHiKDYPnOxfV7QZy4WqXxmjUGIiEhGOoMQEZGMVBAiIpJR0ReEmfUws9lmNtfMBsXOkzYz+72ZLTSzt2NnaSxm1t7MnjOzWWY208zOi50pbWa2oZm9YmZvJJ95aOxMjcHMmpjZa2b2/2JnaSxm9oGZvWVmr5tZVpeTKOoxiGTXuzlU2/UOOL7GrncFxcwOAj4HHnD33WPnaQxmtg2wjbu/amabAjOAnxT4f88GbOzun5tZM+BF4Dx3/0fkaKkyswuBcqCVux8VO09jMLMPgHJ3z/rkwGI/g6jLrncFxd3/TlgYsWi4+7/c/dXk8XJgFhk2oCokHnyefNss+Srofw2aWTvgR8A9sbMUimIviDrtXCeFw8xKgb2AaXGTpC+53PI6sBD4q7sX+me+DbgU+CZ2kEbmwNNmNsPMBmTzBxd7QdRp5zopDGa2CfAYcL67L4udJ23uvtrd9yRsuNXVzAr2kqKZHQUsdPcZsbNEsL+7dwZ6Amcnl5GzotgLQjvXFYnkOvxjwB/d/U+x8zQmd/8MeB7oETlKmvYHeifX48cB3cxsbNxIjcPdFyR/LgQeJ1w6z4piL4i67HoneS4ZsB0DzHL3W2LnaQxmVmJmmyWPWwKHAe/GTZUedx/s7u3cvdX5KuAAAAHySURBVJTw/+Nn3f3EyLFSZ2YbJzdeYGYbA0cAWbtDsagLwt1XAVW73s0Cxrv7zLip0mVmDwEvA7uY2XwzOz12pkawP3AS4V+VrydfvWKHStk2wHNm9ibhH0J/dfeiufWziGwNvGhmbxC2ZZ7o7k9l64cX9W2uIiJSu6I+gxARkdqpIEREJCMVhIiIZKSCEBGRjFQQIiKSkQpCZD2Y2UvJn6VmdkLsPCLZpIIQWQ/uvl/ysBSoV0EkqwmL5CwVhMh6MLOqFVOvBw5MJuFdkCyUd6OZTTezN83sjOT4Q5K9KR4E3kpmwk5M9m1428yOi/ZhRGpoGjuASIEYBFxctQdBsqrmUnfvYmYtgKlm9nRybFdgd3d/38yOARa4+4+S97WOEV4kE51BiKTjCODkZLntaUAboCx57RV3fz95/BZwmJmNMrMD3X1phKwiGakgRNJhwDnuvmfy1cHdq84gvqg6yN3nAD8kFMVIM7s6QlaRjFQQItmxHNi02veTgV8ly4xjZjsnq21+i5ltC3zp7mOBm4DOjRFWpC40BiGSHW8Cq5JVNe8Dbifc2fRqstx4JfCTDO/bA7jRzL4BvgZ+1ShpRepAq7mKiEhGusQkIiIZqSBERCQjFYSIiGSkghARkYxUECIikpEKQkREMlJBiIhIRv8fc+7JHkDa5gcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.78\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    # File对象类似字典\n",
    "    train_data = h5py.File('./dataset/train_catvnoncat.h5', 'r')\n",
    "    test_data = h5py.File('./dataset/test_catvnoncat.h5', 'r')\n",
    "\n",
    "    train_x = np.array(train_data['train_set_x'][:])\n",
    "    train_y = np.array(train_data['train_set_y'][:])\n",
    "\n",
    "    test_x = np.array(test_data['test_set_x'][:])\n",
    "    test_y = np.array(test_data['test_set_y'][:])\n",
    "\n",
    "    # 还原图片\n",
    "    # plt.figure(figsize=(2, 2))\n",
    "    # plt.imshow(train_x[10])\n",
    "    # plt.show()\n",
    "\n",
    "    # reshape数据集\n",
    "    # 特征：64 * 64 * 3 = 12288\n",
    "    # (209, 64, 64, 3) -> (12288, 209)\n",
    "    train_x = train_x.reshape(train_x.shape[0], -1).T\n",
    "    test_x = test_x.reshape(test_x.shape[0], -1).T\n",
    "\n",
    "    # (209, ) -> (1, 209)\n",
    "    train_y = train_y.reshape(1, train_y.shape[0])\n",
    "    test_y = test_y.reshape(1, test_y.shape[0])\n",
    "    \n",
    "    return train_x, train_y, test_x, test_y\n",
    "\n",
    "\n",
    "def init_params(layers, activation='sigmoid', optimizer='gd'):\n",
    "    # 根据传入的数组初始化每一层的W和b\n",
    "    params = {}\n",
    "    layer_num = len(layers)\n",
    "    if optimizer == 'momentum' or optimizer == 'rmsprop':\n",
    "        # momentum中的V和RMSProp中的S\n",
    "        special_cache = {}\n",
    "    \n",
    "    for n in range(1, layer_num):\n",
    "        if activation == 'tanh':\n",
    "            # Xavier初始化，使W的方差等于上一层神经元个数的倒数\n",
    "            params['W'+str(n)] = np.random.randn(layers[n], layers[n-1]) * np.sqrt(1 / layers[n-1])\n",
    "        elif activation == 'ReLU':\n",
    "            # He初始化，ReLU激活函数的解决方案\n",
    "            params['W'+str(n)] = np.random.randn(layers[n], layers[n-1]) * np.sqrt(2 / layers[n-1])\n",
    "        else:\n",
    "            params['W'+str(n)] = np.random.randn(layers[n], layers[n-1]) * 0.01  # 标准正态分布，*0.01将数据保持在±0.03之间\n",
    "        params['b'+str(n)] = np.zeros((layers[n], 1))\n",
    "        \n",
    "        if optimizer == 'momentum' or optimizer == 'rmsprop':\n",
    "            special_cache['dW'+str(n)] = np.zeros((layers[n], layers[n-1]))\n",
    "            special_cache['db'+str(n)] = np.zeros((layers[n], 1))\n",
    "    \n",
    "    for n in range(1, layer_num):\n",
    "        print('W'+str(n)+'=', params['W'+str(n)].shape)\n",
    "        print('b'+str(n)+'=', params['b'+str(n)].shape)\n",
    "    \n",
    "    if optimizer == 'momentum' or optimizer == 'rmsprop':\n",
    "        return params, special_cache\n",
    "    else:\n",
    "        return params\n",
    "\n",
    "\n",
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "\n",
    "def ReLU(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "\n",
    "def tanh(Z):\n",
    "    return np.tanh(Z)\n",
    "    \n",
    "    \n",
    "def sigmoid_grad(A):\n",
    "    # s'(x) = s(x) * (1 - s(x))，所以此处是f(Z)而非Z\n",
    "    return A * (1 - A)\n",
    "\n",
    "\n",
    "def tanh_grad(A):\n",
    "    return 1 - np.square(A)\n",
    "\n",
    "\n",
    "def ReLU_grad(Z):\n",
    "    return np.array(Z > 0)\n",
    "    \n",
    "    \n",
    "def forward_prop(X, params, activation='sigmoid'):\n",
    "    # 每一层WX+b的结果Z和激活后的结果A，用于反向传播\n",
    "    # 要放入原始数据，即A0，否则反向传播会出问题\n",
    "    cache = {'A0': X}  \n",
    "    A = X\n",
    "    layer_num = len(params) // 2  # 用/会得到浮点数\n",
    "    for n in range(1, layer_num):\n",
    "        # b被broadcast，（Ln, 1) -> (Ln, m), m为传入的样本数量\n",
    "        Z = np.dot(params['W'+str(n)], A) + params['b'+str(n)]\n",
    "        A = eval(activation+'(Z)')\n",
    "        cache['Z'+str(n)] = Z\n",
    "        cache['A'+str(n)] = A\n",
    "    \n",
    "    # 最后一层使用sigmoid\n",
    "    Z = np.dot(params['W'+str(layer_num)], A) + params['b'+str(layer_num)]\n",
    "    A = sigmoid(Z)\n",
    "    cache['Z'+str(layer_num)] = Z\n",
    "    cache['A'+str(layer_num)] = A\n",
    "    \n",
    "    return A, cache\n",
    "    \n",
    "    \n",
    "def mean_square_loss(A_last, Y):\n",
    "    m = Y.shape[1]\n",
    "    return np.sum((A_last - Y) * (A_last - Y)) / m / 2\n",
    "    \n",
    "\n",
    "def back_prop(Y, params, cache, activation='sigmoid'):\n",
    "    layer_num = len(params) // 2\n",
    "    m = Y.shape[1]\n",
    "    grads = {}  # 保存所有梯度\n",
    "    \n",
    "    # 最后一层的Z和A\n",
    "    A_last = cache['A'+str(layer_num)]\n",
    "    Z_last = cache['Z'+str(layer_num)]\n",
    "    \n",
    "    # 数组的形状\n",
    "    # W(N(L), N(L-1)), 此处W是当前层左侧边权重，这样Z(L) = W(L)A(L-1)+b(L)不会矛盾\n",
    "    # X/A/Z(N(L), m), b(N(L), 1)\n",
    "    \n",
    "    # 公式\n",
    "    # (最后一层) dZ = (A(L) - y) * f'(Z(L)) /// (N(L), m), (N(L), m--广播后)\n",
    "    # (非最后一层) dZ = (W(L+1).T) dZ(L+1) * f'(Z(L)) /// (N(L), N(L+1)), (N(L+1), m), (N(L), m--广播后)\n",
    "    # 此处W(L+1)是指Z所在神经元右侧的W\n",
    "    # dW = (1/m) * (dJ/dZ(L)) (A(L-1).T) /// (N(L), m), (m, N(L-1))\n",
    "    # db = (1/m) * sum(dJ/dZ(L)) /// (N(L), m) -> (N(L), 1)\n",
    "    \n",
    "    # 最后一层（输出层）dJ/dZ，且使用sigmoid\n",
    "    dZ = (A_last - Y) * sigmoid_grad(A_last)\n",
    "    # 最后一层dJ/dW和dJ/db\n",
    "    grads['dW'+str(layer_num)] = np.dot(dZ, cache['A'+str(layer_num-1)].T) / m\n",
    "    # 在第二维求和（横向），保持二维\n",
    "    grads['db'+str(layer_num)] = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "    \n",
    "    for n in range(layer_num-1, 0, -1):\n",
    "        if activation == 'ReLU':\n",
    "            dZ = np.dot(params['W'+str(n+1)].T, dZ) * eval(activation + '_grad' + \"(cache['Z'+str(n)])\")\n",
    "        else:\n",
    "            dZ = np.dot(params['W'+str(n+1)].T, dZ) * eval(activation + '_grad' + \"(cache['A'+str(n)])\")\n",
    "        grads['dW'+str(n)] = np.dot(dZ, cache['A'+str(n-1)].T) / m\n",
    "        grads['db'+str(n)] = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "    \n",
    "    return grads\n",
    "        \n",
    "\n",
    "def gradient_descent(grads, params, lr):\n",
    "    # W = W - lr * dW，b相同\n",
    "    layer_num = len(params) // 2\n",
    "    for n in range(1, layer_num+1):\n",
    "        params['W'+str(n)] = params['W'+str(n)] - lr * grads['dW'+str(n)]\n",
    "        params['b'+str(n)] = params['b'+str(n)] - lr * grads['db'+str(n)]\n",
    "        \n",
    "    return params\n",
    "\n",
    "\n",
    "def momentum(grads, params, V, lr, momentum=0.9):\n",
    "    # V = momentum * V + dW(db)\n",
    "    # W = W - lr * V\n",
    "    layer_num = len(params) // 2\n",
    "    for n in range(1, layer_num+1):\n",
    "        V['dW'+str(n)] = V['dW'+str(n)] * momentum + grads['dW'+str(n)]\n",
    "        params['W'+str(n)] = params['W'+str(n)] - lr * V['dW'+str(n)]\n",
    "        \n",
    "        V['db'+str(n)] = V['db'+str(n)] * momentum + grads['db'+str(n)]\n",
    "        params['b'+str(n)] = params['b'+str(n)] - lr * V['db'+str(n)]\n",
    "        \n",
    "    return params, V\n",
    "\n",
    "\n",
    "def RMSProp(grads, params, S, lr, decay=0.9, epsilon=1e-8):\n",
    "    # S = decay * S + (1 - decay) * square(dW)\n",
    "    # W = W - lr / sqrt(S + epsilon) * dW\n",
    "    layer_num = len(params) // 2\n",
    "    for n in range(1, layer_num+1):\n",
    "        S['dW'+str(n)] = S['dW'+str(n)] * decay + np.square(grads['dW'+str(n)]) * (1 - decay)\n",
    "        params['W'+str(n)] = params['W'+str(n)] - lr / np.sqrt(S['dW'+str(n)] + epsilon) * grads['dW'+str(n)]\n",
    "        \n",
    "        S['db'+str(n)] = S['db'+str(n)] * decay + np.square(grads['db'+str(n)]) * (1 - decay)\n",
    "        params['b'+str(n)] = params['b'+str(n)] - lr / np.sqrt(S['db'+str(n)] + epsilon) * grads['db'+str(n)]\n",
    "        \n",
    "    return params, S\n",
    "\n",
    "\n",
    "def dict_to_col_vector(grads_or_params, if_grads):\n",
    "    # 将梯度或参数字典转为列向量\n",
    "    layer_num = len(grads_or_params) // 2\n",
    "    prefix = ''\n",
    "    if if_grads == True:\n",
    "        prefix = 'd'\n",
    "    \n",
    "    col_vec = np.array([[0]]) # 为了方便连接\n",
    "    for n in range(1, layer_num+1):\n",
    "        dW_vec = np.reshape(grads_or_params[prefix+'W'+str(n)], (-1, 1))\n",
    "        db_vec = np.reshape(grads_or_params[prefix+'b'+str(n)], (-1, 1))\n",
    "        col_vec = np.concatenate((col_vec, dW_vec, db_vec), axis=0)\n",
    "    \n",
    "    return col_vec[1:]\n",
    "\n",
    "\n",
    "def col_vector_to_dict(col_vec, params_src):\n",
    "    # 根据原始字典params_src的形状将col_vec复原\n",
    "    layer_num = len(params_src) // 2\n",
    "    reshaped = {}\n",
    "    \n",
    "    idx_start = 0\n",
    "    idx_end = 0\n",
    "    for n in range(1, layer_num+1):\n",
    "        # W\n",
    "        row = params_src['W'+str(n)].shape[0]\n",
    "        col = params_src['W'+str(n)].shape[1]\n",
    "        idx_end = idx_start + row * col\n",
    "        reshaped['W'+str(n)] = col_vec[idx_start:idx_end].reshape((row, col))\n",
    "        idx_start = idx_end\n",
    "        # b\n",
    "        row = params_src['b'+str(n)].shape[0]\n",
    "        col = params_src['b'+str(n)].shape[1]\n",
    "        idx_end = idx_start + row * col\n",
    "        reshaped['b'+str(n)] = col_vec[idx_start:idx_end].reshape((row, col))\n",
    "        idx_start = idx_end\n",
    "    \n",
    "    return reshaped\n",
    "\n",
    "\n",
    "def L2(vec):\n",
    "    return np.sqrt(np.sum(np.square(vec)))\n",
    "\n",
    "\n",
    "def gradient_check(X, Y, grads, params, epsilon=1e-6, activation='sigmoid'):\n",
    "    grads_vec = dict_to_col_vector(grads, True) # 使用导数定义求得的梯度值\n",
    "    params_vec = dict_to_col_vector(params, False)\n",
    "    grads_check = np.zeros(grads_vec.shape)\n",
    "    params_num = params_vec.shape[0]\n",
    "    \n",
    "    # 整体的梯度检验，也可以对每个W和b进行单独检验\n",
    "    for i in range(params_num):\n",
    "        if i % 1000 == 0:\n",
    "            print('checking gradient for parameters, checked:', i)\n",
    "        # 对 参数+epsilon 求损失J\n",
    "        params_vec[i][0] = params_vec[i][0] + epsilon\n",
    "        A_last, _ = forward_prop(X, col_vector_to_dict(params_vec, params), activation)\n",
    "        J_plus_e = mean_square_loss(A_last, Y)\n",
    "        # 对 参数-epsilon 求损失J\n",
    "        params_vec[i][0] = params_vec[i][0] - 2 * epsilon  # 相当于原参数减去epsilon\n",
    "        A_last, _ = forward_prop(X, col_vector_to_dict(params_vec, params), activation)\n",
    "        J_minus_e = mean_square_loss(A_last, Y)\n",
    "        # 复原params_vec\n",
    "        params_vec[i][0] = params_vec[i][0] + epsilon\n",
    "        \n",
    "        grads_check[i][0] = (J_plus_e - J_minus_e) / epsilon / 2\n",
    "    \n",
    "#         curr_grad = (J_plus_e - J_minus_e) / epsilon / 2\n",
    "#         diff = abs(grads_vec[i][0] - curr_grad) / max(1, abs(grads_vec[i][0]), abs(curr_grad))\n",
    "#         if diff > 1e-4:\n",
    "#             print(\"Gradient check failed, first error index %s.\" % str(i))\n",
    "#             print(\"The diff is\", diff)\n",
    "#             print(\"Your gradient: %.9f \\t Numerical gradient: %.9f\" % (grads_vec[i][0], curr_grad))\n",
    "#             return\n",
    "    diff = L2(grads_check - grads_vec) / (L2(grads_check) + L2(grads_vec))\n",
    "    print(\"The diff is\", diff)\n",
    "\n",
    "    \n",
    "def divide_data(train_x, train_y, batch_size=64):\n",
    "    m = train_x.shape[1]\n",
    "    # 打乱数据集\n",
    "    permutation = np.random.permutation(m).tolist() # 获得一个0-m-1的随机序列\n",
    "    X_shuffled = train_x[:, permutation]\n",
    "    y_shuffled = train_y[:, permutation]\n",
    "    mini_batches = []\n",
    "    # 分批次\n",
    "    batch_num = m // batch_size\n",
    "    for i in range(batch_num):\n",
    "        mini_batch_X = X_shuffled[:, i*batch_size : (i+1)*batch_size]\n",
    "        mini_batch_y = y_shuffled[:, i*batch_size : (i+1)*batch_size]\n",
    "        mini_batches.append((mini_batch_X, mini_batch_y))\n",
    "    # 多余的样本\n",
    "    if m % batch_size != 0:\n",
    "        mini_batch_X = X_shuffled[:, (i+1)*batch_size:]\n",
    "        mini_batch_y = y_shuffled[:, (i+1)*batch_size:]\n",
    "        mini_batches.append((mini_batch_X, mini_batch_y))\n",
    "        \n",
    "    return mini_batches\n",
    "    \n",
    "\n",
    "def train(layers, train_x, train_y, batch_size=64, grads_check=False, epoches=500, lr=0.01, activation='sigmoid', optimizer='gd'):\n",
    "    # 初始化w和b\n",
    "    if optimizer == 'momentum' or optimizer == 'rmsprop':\n",
    "        params, special_cache = init_params(layers, activation, optimizer)\n",
    "    else:\n",
    "        params = init_params(layers, activation, optimizer)\n",
    "\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epoches):\n",
    "        # 应该每个epoch都对数据集进行随机打乱\n",
    "        mini_batches = divide_data(train_x, train_y, batch_size)\n",
    "        \n",
    "        for mini_batch in mini_batches:\n",
    "            mini_batch_X, mini_batch_y = mini_batch\n",
    "            # 前向传播, A_last(1, 209)\n",
    "            A_last, cache = forward_prop(mini_batch_X, params, activation)\n",
    "\n",
    "            # 计算loss\n",
    "            loss = mean_square_loss(A_last, mini_batch_y)\n",
    "            if epoch % 500 == 0:\n",
    "                print('epoches =', epoch, ' loss =', loss)\n",
    "                losses.append(loss)\n",
    "\n",
    "            # 反向传播\n",
    "            grads = back_prop(mini_batch_y, params, cache, activation)\n",
    "#             if grads_check and i == iters - 1:\n",
    "#                 gradient_check(train_x, train_y, grads, params, activation)\n",
    "\n",
    "            # 梯度下降更新参数\n",
    "            if optimizer == 'momentum':\n",
    "                params, V = momentum(grads, params, special_cache, lr)\n",
    "            elif optimizer == 'rmsprop':\n",
    "                params, S = RMSProp(grads, params, special_cache, lr)\n",
    "            else:\n",
    "                params = gradient_descent(grads, params, lr)\n",
    "\n",
    "            # 检查梯度消失问题\n",
    "#             if i > 0 and i % 2000 == 0:\n",
    "#                 for n in range(1, len(layers)):\n",
    "#                     # 激活值的均值和标准差\n",
    "#                     print('A' + str(n) + ' mean:', np.mean(cache['A'+str(n)][:,1]), \n",
    "#                           'A' + str(n) + ' std:', np.std(cache['A'+str(n)][:,1]))\n",
    "#                     # 梯度接近0的参数统计\n",
    "#                     print('dW ' + str(n) + ' <e-8:', np.sum(grads['dW'+str(n)] < 1e-8), \n",
    "#                           'total:', grads['dW' + str(n)].shape[0] * grads['dW' + str(n)].shape[1])\n",
    "    \n",
    "    plt.plot(losses, 'r')\n",
    "    plt.xlabel('iters')\n",
    "    plt.ylabel('loss')\n",
    "    plt.show()\n",
    "    \n",
    "    return params\n",
    "\n",
    "\n",
    "def predict(X, Y, params, activation='sigmoid'):\n",
    "    m = X.shape[1] # 样本数量\n",
    "    A_last, _ = forward_prop(X, params, activation)\n",
    "    result = np.zeros(A_last.shape)\n",
    "    for i in range(A_last.shape[1]):\n",
    "        if A_last[0][i] >= 0.5:\n",
    "            result[0][i] = 1\n",
    "        else:\n",
    "            result[0][i] = 0\n",
    "    acc = np.sum(result==Y) / m\n",
    "    print('accuracy =', acc)\n",
    "    \n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    train_x, train_y, test_x, test_y = load_data()\n",
    "    # 归一化\n",
    "    train_x = train_x / 255.0\n",
    "    test_x = test_x / 255.0\n",
    "    \n",
    "    layers = [12288, 10, 3, 2, 1]\n",
    "#     layers = [12288, 10, 1]\n",
    "    params = train(layers, train_x, train_y, batch_size=209, epoches=3000, lr=0.0001, activation='ReLU', optimizer='rmsprop')\n",
    "    predict(test_x, test_y, params, activation='ReLU')\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
